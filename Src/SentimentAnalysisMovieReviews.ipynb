{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3475s\u001b[0m 7s/step - accuracy: 0.7759 - loss: 0.4370 - val_accuracy: 0.8800 - val_loss: 0.3187\n",
      "Epoch 2/5\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3658s\u001b[0m 7s/step - accuracy: 0.9309 - loss: 0.1805 - val_accuracy: 0.8800 - val_loss: 0.3371\n",
      "Epoch 3/5\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5434s\u001b[0m 11s/step - accuracy: 0.9610 - loss: 0.1161 - val_accuracy: 0.9000 - val_loss: 0.3534\n",
      "Epoch 4/5\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6427s\u001b[0m 13s/step - accuracy: 0.9778 - loss: 0.0733 - val_accuracy: 0.8700 - val_loss: 0.4705\n",
      "Epoch 5/5\n",
      "\u001b[1m499/499\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5025s\u001b[0m 10s/step - accuracy: 0.9868 - loss: 0.0439 - val_accuracy: 0.8700 - val_loss: 0.5129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24494d953c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, GlobalMaxPool1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r'..\\Data\\archive\\IMDB Dataset.csv')\n",
    "\n",
    "# The first 100 reviews are going to be used for testing\n",
    "\n",
    "data=data.sample(frac=1)\n",
    "data_train = data.iloc[100:]\n",
    "data_test = data.iloc[:100]\n",
    "\n",
    "# Preprocess the data\n",
    "\n",
    "def custom_standardization(input_data):     #format the text removing HTML\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation), '') # Removes all punctuation from the text.\n",
    "\n",
    "max_features = 10000\n",
    "\n",
    "tokenizer=tokens = Tokenizer(num_words=max_features)\n",
    "\n",
    "tokenizer.fit_on_texts(data_train['review'])\n",
    "\n",
    "# convert the text to sequences\n",
    "x_train = tokenizer.texts_to_sequences(data_train['review'])\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(data_test['review'])\n",
    "\n",
    "# adding padding to get all to the same length\n",
    "\n",
    "maxlen= max([len(x) for x in x_train])\n",
    "\n",
    "x_train= sequence.pad_sequences(x_train, maxlen=maxlen, padding ='post')\n",
    "x_test= sequence.pad_sequences(x_test, maxlen=maxlen, padding ='post')\n",
    "\n",
    "\n",
    "# obtaining the labels (transforming them into numbers)\n",
    "\n",
    "y_train = data_train['sentiment'].map({'positive':1, 'negative':0}).values\n",
    "\n",
    "y_test = data_test['sentiment'].map({'positive':1, 'negative':0}).values\n",
    "\n",
    "# Define the model\n",
    "\n",
    "embed_size = 128\n",
    "\n",
    "input= Input(shape=(maxlen,))\n",
    "\n",
    "model= Sequential([\n",
    "    Embedding(max_features, embed_size), #embedding layer: 1000 words and 128 features (converts the input into vectors of fixed size and vocabulary)\n",
    "    LSTM(60, return_sequences=True),  # identify the important features in the text and ignoring the unimportant ones (60 neurons, each token generates an output)\n",
    "    GlobalMaxPool1D(), # takes only the biggest value for each feature coming from the output of the LSTM\n",
    "    Dense(50, activation='relu'), #50 neurons in the hidden layer with relu activation function\n",
    "    Dropout(0.1), #to prevent overfitting\n",
    "    Dense(2, activation='sigmoid') # 2 classes in the final layer: positive and negative\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416ms/step - accuracy: 0.8600 - loss: 0.3673\n",
      "Test loss: 0.3673033118247986 - test accuracy: 0.8600000143051147\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step\n",
      "Review: This movie is fantastic! I really enjoyed it and I would recommend it to everyone! -- Sentiment: Positive\n",
      "Review: That acting was terrible! I can't believe I wasted my time watching this movie! -- Sentiment: Negative\n",
      "Review: The film didn't meet my expectations. I was very disappointed with it. -- Sentiment: Negative\n",
      "Review: The film didn't start for 30 minutes but it was worth the wait. -- Sentiment: Positive\n",
      "Review: The movie was good but the ending was terrible. -- Sentiment: Negative\n",
      "Review: I love this movie -- Sentiment: Positive\n",
      "Review: I hate this movie -- Sentiment: Negative\n",
      "Review: I don't like this movie, it was boring -- Sentiment: Negative\n",
      "Review: I don't hate this movie, it was almost good -- Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 3\n",
    "\n",
    "# split the test set into evaluation and validation set\n",
    "\n",
    "validation_split = 0.5\n",
    "validation_size= int(len(x_test)*validation_split)\n",
    "\n",
    "x_val = x_test[:validation_size]\n",
    "y_val = y_test[:validation_size]\n",
    "\n",
    "x_eval = x_test[validation_size:]\n",
    "y_eval = y_test[validation_size:]\n",
    "\n",
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "score = model.evaluate(x_eval, y_eval, batch_size=batch_size)    #easy to do since dataset already formatted\n",
    "\n",
    "print(f'Test loss: {score[0]} - test accuracy: {score[1]}')\n",
    "\n",
    "\n",
    "# Manual test with some handwriten reviews\n",
    "\n",
    "sample_reviews_plain = [\n",
    "    'This movie is fantastic! I really enjoyed it and I would recommend it to everyone!',\n",
    "    'That acting was terrible! I can\\'t believe I wasted my time watching this movie!',\n",
    "    'The film didn\\'t meet my expectations. I was very disappointed with it.',\n",
    "    'The film didn\\'t start for 30 minutes but it was worth the wait.',\n",
    "    'The movie was good but the ending was terrible.',\n",
    "    'I love this movie',\n",
    "    'I hate this movie',\n",
    "    'I don\\'t like this movie, it was boring',\n",
    "    'I don\\'t hate this movie, it was almost good'\n",
    "]\n",
    "\n",
    "# Format the reviews\n",
    "\n",
    "sample_reviews = tokenizer.texts_to_sequences(sample_reviews_plain)\n",
    "sample_reviews = sequence.pad_sequences(sample_reviews, maxlen=maxlen, padding='post')\n",
    "\n",
    "predictions = model.predict(sample_reviews)\n",
    "predicted_sentiments = (predictions > 0.5).astype(int)\n",
    "\n",
    "for review, prediction in zip(sample_reviews_plain, predicted_sentiments):\n",
    "    print(f'Review: {review} -- Sentiment: {\"Positive\" if prediction[1] == 1 else \"Negative\"}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MovieRewSentimentAnalysisEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
